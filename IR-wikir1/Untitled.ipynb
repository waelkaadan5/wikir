{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "27bedb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ir_datasets in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (1.23.5)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (0.2.2)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (0.1.5)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (0.2.3)\n",
      "Requirement already satisfied: lxml>=4.5.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (4.9.1)\n",
      "Requirement already satisfied: lz4>=3.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (3.1.3)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (4.64.1)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (2.6)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: ijson>=3.1.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (3.2.0.post0)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (0.1.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (4.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ir_datasets) (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.22.0->ir_datasets) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.22.0->ir_datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.22.0->ir_datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.22.0->ir_datasets) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.38.0->ir_datasets) (0.4.6)\n",
      "Requirement already satisfied: cbor>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "afa4fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, ISRIStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7528d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] If you have a local copy of https://ciir.cs.umass.edu/downloads/Antique/antique-collection.txt, you can symlink it here to avoid downloading it again: C:\\Users\\hp\\.ir_datasets\\downloads\\684f7015aff377062a758e478476aac8\n",
      "[INFO] [starting] https://ciir.cs.umass.edu/downloads/Antique/antique-collection.txt\n",
      "[INFO] [finished] https://ciir.cs.umass.edu/downloads/Antique/antique-collection.txt: [01:19] [93.6MB] [1.18MB/s]\n",
      "                                                                                               \r"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mir_datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ir_datasets\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantique\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdocs_iter():\n\u001b[0;32m      4\u001b[0m     query\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\util\\__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:92\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     cols \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m     num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39m_fields)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:30\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctxt\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdlc\u001b[38;5;241m.\u001b[39mstream()))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\n\u001b[1;32m---> 30\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"antique\")\n",
    "for query in dataset.docs_iter():\n",
    "    query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0203de",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mir_datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ir_datasets\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantique/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdocs_iter():\n\u001b[0;32m      4\u001b[0m     query\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\util\\__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:92\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     cols \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m     num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39m_fields)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:30\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctxt\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdlc\u001b[38;5;241m.\u001b[39mstream()))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\n\u001b[1;32m---> 30\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "antique_train_df = pd.read_csv('drive/My Drive/antique_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6611c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "89f35d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 =ir_datasets.load(\"antique/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c561d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "Df = pd.read_csv(\"antique_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2466cc98",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoc_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Dataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    " print(dataset1['doc_id'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2112e3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "067d88a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3990512</th>\n",
       "      <th>how can we get concentration onsomething?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>714612</td>\n",
       "      <td>Why doesn't the water fall off  earth if it's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2528767</td>\n",
       "      <td>How do I determine the charge of the iron ion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>821387</td>\n",
       "      <td>I have mice.How do I get rid of them humanely?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1880028</td>\n",
       "      <td>What does \"see Leaflet\" mean on Ept Pregnancy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4448097</td>\n",
       "      <td>what is innate immunity?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2192891</td>\n",
       "      <td>how are braces put on and do they hurt a lot?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>4406669</td>\n",
       "      <td>What do you order at Taco Bell?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1582877</td>\n",
       "      <td>why do we go to school if in the real world we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1340574</td>\n",
       "      <td>Why do some people only go to church on Easter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1971899</td>\n",
       "      <td>what is masturbat***?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3990512          how can we get concentration onsomething?\n",
       "0     714612  Why doesn't the water fall off  earth if it's ...\n",
       "1    2528767  How do I determine the charge of the iron ion ...\n",
       "2     821387     I have mice.How do I get rid of them humanely?\n",
       "3    1880028  What does \"see Leaflet\" mean on Ept Pregnancy ...\n",
       "4    4448097                           what is innate immunity?\n",
       "..       ...                                                ...\n",
       "194  2192891      how are braces put on and do they hurt a lot?\n",
       "195  4406669                    What do you order at Taco Bell?\n",
       "196  1582877  why do we go to school if in the real world we...\n",
       "197  1340574  Why do some people only go to church on Easter...\n",
       "198  1971899                              what is masturbat***?\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = pd.read_table('test1/queries.txt')\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4ef0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(sentence):\n",
    "  \"\"\" we join the splitted sentence here because RegeX requirs connected text,\n",
    "    then re-split it to pass it for the other functions \"\"\"\n",
    "\n",
    "  new_str = re.sub(r'[.|?|@|$|%|&]', ' ', sentence)\n",
    "  return new_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f290c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b75c7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_Words(sentence):\n",
    "  return [word for word in sentence if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a216396",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(sentence):\n",
    "  return [stemmer.stem(i) for i in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c20c63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(sentence):\n",
    "  return [lemmatizer.lemmatize(i) for i in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd16d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, rm_stop_words = True, rm_special_chars = True, stemmer = True, lemmatizer = True):\n",
    "  \n",
    "  \"\"\" by defualt it will take in count all the preprocessing steps,\n",
    "    just in case you don't need one of them you can call its param and set it to False.\n",
    "      input: the doc_text, then split it and pass it to the desired preprocessing functions\n",
    "      output: join the preprocessed splitted text \"\"\"\n",
    "\n",
    "  \"\"\" the resoan for making the underblow Split is: \n",
    "    all the preprocessing funcions require split the sentence, \n",
    "    whith that being said we will split it once and then re-join it in the output \"\"\"\n",
    "\n",
    "  sentence = str(sentence).split()  \n",
    "  if rm_special_chars:\n",
    "    sentence = remove_special_chars(\" \".join(sentence))\n",
    "  \n",
    "  if rm_stop_words:\n",
    "    sentence = remove_stop_Words(sentence)\n",
    "\n",
    "  if lemmatizer:\n",
    "    sentence = lemmatization(sentence)\n",
    "\n",
    "  if stemmer:\n",
    "    sentence = stemming(sentence)\n",
    "\n",
    "  return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1548e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "antique_train_df['cleaned_text'][:100] = Df['doc_text'][:100].apply(lambda sentence: preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8859859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "Df_cleaned = defaultdict(list)\n",
    "Df_cleaned['cleaned_text'][:100] = Df['doc_text'][:100].apply(lambda sentence: preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d3591f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericDoc(doc_id='2020338_0', text=\"A small group of politicians believed strongly that the fact that Saddam Hussien remained in power after the first Gulf War was a signal of weakness to the rest of the world, one that invited attacks and terrorism. Shortly after taking power with George Bush in 2000 and after the attack on 9/11, they were able to use the terrorist attacks to justify war with Iraq on this basis and exaggerated threats of the development of weapons of mass destruction. The military strength of the U.S. and the brutality of Saddam's regime led them to imagine that the military and political victory would be relatively easy.\")\n",
      "GenericDoc(doc_id='2020338_1', text='Because there is a lot of oil in Iraq.')\n",
      "GenericDoc(doc_id='2020338_2', text='It is tempting to say that the US invaded Iraq because it has lots of oil, but the US is not a country in a deep economic problem that capturing other countryâ€™s oil is an actual need for survival. It is more likely that the Iraq invading Kuwait scenario would fall under that assumption.. I think that the US government has come to a conclusion that we are on the verge of a war of religions, or more likely ideologies. It would be presumptuous to try and determent a one cause to the coming war. . I think that the world wide spread of the media with its many forms (Cable, Satellite, Internet, etc.)  have pushed the Moslem regimes to the extreme, fearing that secularity and democratic influence is penetrating their country and will result in an up raising against them. One of the best way to maintain the power that you have and even gain more of it, is by hatred. When the common man is occupied hating an outside enemy, its hatred is kept out side the county and would not be directed towards the regime. . So- I believe that the US understands that the fanatic Moslem regimes have already started a war on the democratic world and now is the time to try a fight it.. . So why invade Iraq? Because it is a huge, week Moslem country that thought to be easy to defeat. . This is exactly the same reason why Afghanistan was first and Syria is next in line.')\n",
      "GenericDoc(doc_id='2020338_3', text='I think Yuval is pretty spot on. It\\'s a proving ground and a focal point for terror activity that\\'s not on American soil. And, because no one liked Saddam Hussein, no other countries (even in the Middle East) were about to rise up and join his side.. . Rabid speculation: now the Pentagon has a model that says it takes ~5 years, ~$200B and ~2,000 casualties to \"rebuild\" a dictatorship into a democracy. Who\\'s next on the list?')\n",
      "GenericDoc(doc_id='2874684_0', text='Call an area apiarist.  They should be able to help you and would most likely remove them at no charge in exchange for the hive.  The bees have value and they now belong to you.')\n",
      "GenericDoc(doc_id='2874684_1', text=\"Don't try this yourself but this is what the old fashioned way is - done in some villages in other parts of the world.. . The guy who dismantles, wears a mask with holes at the eyes so he can see, carries a big sack with burning coals in it.  He also wraps his hands with thick cloths, carries an empty bag to collect the honey in it.. He then collects the honey in the empty bag, pulls the hive into the bag of coals.. . If he just wants to collect the honey, he would leave after collecting the honey - but the bag of coals is used to choke & burn the bees if he wanted to get rid of the hive.. . That's a bit like coding in COBOL & assembly :)\")\n",
      "GenericDoc(doc_id='4193114_0', text=\"There's a general belief in Europe (and in fact elsewhere in the world, such as China) that it's unhealthy to drink water with a meal because it dilutes the digestive juices. Wine is OK because it stimulates digestion, and mineral waters are also considered digestives.. . In general, Europeans are much more concerned with the digestive process than Americans.\")\n",
      "GenericDoc(doc_id='4193114_1', text='I\\'d like to take a different approach in answering this. I think geographical location and higher availability of other drinks probably is a bigger reason that water is not consumed as much by people in certain regions, including Europe. I believe and many visitors have commented that children drink less water and more soda in USA.  This is primarily due to the fact that soda companies advertise a lot, making it more attractive. Secondly, SODA IS ATTRACTIVE, and according to a few medical experts it\\'s even addictive (yes, caffeine is addictive to some extent).  . . Secondly the cold weather in upper northern hemisphere in general is more of a reason that ppl drink less water and more of other drinks.  For example if you are from a tropical country or from a very humid place, you would probably & naturally  be drinking more water rather than if you are from   a country like Austria, Switzerland, Denmark, Finland.. . Another reason is that people being less immune to impure water (like tap water) and being more dependent on bottled water makes the water less affordable and less available.  If you need to pay  extra or need to work harder to get the commodity, you may resort to cheaper commodity or a near by commodity.. . Another reason I also feel is the commute using Cars rather than walking or cycling.  If you walk more to commute (like walk to train stations, walk a few blocks to get groceries, etc.) then you are more likely to drink water when compared to driving to every place you go and exert yourself much less.  In many other countries people walk, walk and walk for several small things.. . Lack of education and awareness is also a reason for not consuming enough water. Not many people (except for few educated ones)know that 80% of our body is water.  Even if they knew, ppl forget that fact easily and resort to \"other drinks\".. . There are simply several reasons or combination of reasons for not consuming enough water by the people in general.. . Finally, after having said all this,. Beer Happens!')\n",
      "GenericDoc(doc_id='1908421_0', text=\"hybrid cars save energy in two ways: 1.by storing energy from the breaks2.by engaging the gas motor only in it's most effective work RPM meaning that the system will engage the gas motor mostly at coursing speed (close to the most efficient motor RPM)Having said the above, driving in the city almost never gets you to the efficient RPM.Also- it is true that the same energy that is being stored in the cars' battery needed to be generated by chemic energy. The cool thing about it is that the pollution can be outside of the city and not in the city. (Like in San Francisco, the electric buses generate the same pollution as a gas buses it is just not in the city.You can also add that electric engines are much quieter then gas engines.So- all in all- I the hybrid benefits are more visible in city driving\")\n",
      "GenericDoc(doc_id='1908421_1', text='The gas mileage for some hybrids is better in the city because they spend more time using their electric motor instead of their gas engine.  Most hybrids have battery packs which are charged by what is called \"generative breaking\".  Instead of slowing down with traditional braking, these cars store some of the energy from braking into a battery pack.  Then when the light changes and the car moves again, the energy stored in the battery can be used to move the car, or assist the engine in moving the car.. . In the city this effect can be more pronounced because of more frequent stopping and starting.')\n",
      "GenericDoc(doc_id='1908421_2', text='There are the two reasons mentioned already - using the energy from the breaks and using the gas engine only when it can work efficiently.. . One more thing to note is that for several cars, the hybrid model of a car has a smaller gas engine than the regular models (e.g. for Honda Civic the gas engine for the hybrid model is 1.3L, which is not available in the regular models). So even without the other things, this engine will get better gas mileage than the larger engines.')\n",
      "GenericDoc(doc_id='3608897_0', text='In general it means that in a very high speed (also apply for low speed but hard to measure) , a Mass turns into Energy and vise versa. The conversion rate is C2 (C square).')\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mir_datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ir_datasets\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantique/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdocs_iter():\n\u001b[0;32m      4\u001b[0m        \u001b[38;5;28mprint\u001b[39m(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\util\\__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:92\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     cols \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m     num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39m_fields)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:30\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctxt\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdlc\u001b[38;5;241m.\u001b[39mstream()))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\n\u001b[1;32m---> 30\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "for doc in dataset.docs_iter():\n",
    "       print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e523a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e31fed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mir_datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ir_datasets\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantique/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdocs_iter():\n\u001b[0;32m      4\u001b[0m     doc\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\util\\__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:92\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     cols \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m     num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39m_fields)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ir_datasets\\formats\\tsv.py:30\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctxt\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdlc\u001b[38;5;241m.\u001b[39mstream()))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\n\u001b[1;32m---> 30\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2206: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "for doc in dataset.docs_iter():\n",
    "    doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347bbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
